<!DOCTYPE html>
<html lang="en">
<head>

        <title>Expectation-Maximization(EM) 算法 - ~/blog</title>
        <meta charset="utf-8" />
        <link href="http://blog.tomtung.com/feed/all.atom.xml" type="application/atom+xml" rel="alternate" title="~/blog Full Atom Feed" />
        <link href="http://blog.tomtung.com/feed/atom.xml" type="application/atom+xml" rel="alternate" title="~/blog Atom Feed" />
        <link href="http://blog.tomtung.com/feed/index.html" type="application/rss+xml" rel="alternate" title="~/blog RSS Feed" />


        <!-- Mobile viewport optimized: j.mp/bplateviewport -->
        <meta name="viewport" content="width=device-width,initial-scale=1, maximum-scale=1">

        <link rel="stylesheet" type="text/css" href="http://blog.tomtung.com/theme/css/gumby.css" />

        <script src="http://blog.tomtung.com/theme/js/libs/modernizr-2.6.2.min.js"></script>
        <script type="text/javascript"
                src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

            <script type="text/javascript">

              var _gaq = _gaq || [];
              _gaq.push(['_setAccount', 'UA-12852556-1']);
              _gaq.push(['_trackPageview']);

              (function() {
                var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
                ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
                var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
              })();

            </script>
</head>

<body id="index" class="home">
  <div id="navigation" class="navbar">
    <a href="#" gumby-trigger="#navigation &gt; ul" class="toggle"><i class="icon-menu"></i></a>
    <span id="navbar-title"><a href="http://blog.tomtung.com"> ~/blog </a></span>
    <ul id="navbar-items">
        <li ><a href="http://blog.tomtung.com/archive">Archive</a></li>
        <li ><a href="http://blog.tomtung.com/tags">Tags</a></li>
      
        <li><a href="http://blog.tomtung.com/feed/atom.xml">Feed</a></li>
    </ul>
  </div>

  <div id="content-container" class="container">
<section id="content" class="body">
  <header class="post-header">
    <div>
      <time datetime="2011-10-13T00:54:00">
        2011-10-13
      </time>
    </div>
    <h2 class="entry-title">
      <a href="http://blog.tomtung.com/2011/10/em-algorithm/" rel="bookmark">
        Expectation-Maximization(<span class="caps">EM</span>)&nbsp;算法
      </a>
    </h2>
    <div class="tag-row">
        <span class="tag-row-icon"><i class="icon-tag"></i></span>
          <span class="small default oval btn">
            <a href="http://blog.tomtung.com/tag/statistics.html">statistics</a>
          </span>
          <span class="small default oval btn">
            <a href="http://blog.tomtung.com/tag/algorithm.html">algorithm</a>
          </span>
          <span class="small default oval btn">
            <a href="http://blog.tomtung.com/tag/machine-learning.html">machine learning</a>
          </span>
    </div>
   
  </header>

  <div class="post-content">
    <p>Expectation-Maximization 算法是统计学中用来给带<a class="reference external" href="http://en.wikipedia.org/wiki/Latent_variable">隐含变量</a>的模型做最大似然（和最大后验概率）的一种方法。<span class="caps">EM</span> 的应用特别广泛，经典的比如做概率密度估计用的 <a class="reference external" href="http://en.wikipedia.org/wiki/Gaussian_mixture_model">Gaussian Mixture Model</a>。这两天我或许还会写 p 的笔记放上来，也是 <span class="caps">EM</span>&nbsp;应用的例子。</p>
<p>下面我会先解释 <span class="caps">EM</span>&nbsp;算法要解决的问题，然后直接给出算法的过程，最后再说明算法的正确性。</p>
<div class="section" id="id1">
<h2>问题</h2>
<p>首先我们定义要解决的问题。给定一个训练集 <span class="math">\(X=\{x^{(1)},&#8230;,x^{(m)}\}\)</span>，我们希望拟合包含隐含变量 <span class="math">\(z\)</span> 的模型 <span class="math">\(P(x,z;\theta)\)</span> 中的参数 <span class="math">\(\theta\)</span>。根据模型的假设，每个我们观察到的 <span class="math">\(x^{(i)}\)</span> 还对应着一个我们观察不到的隐含变量 <span class="math">\(z^{(i)}\)</span>，我们记 <span class="math">\(Z=\{z^{(1)},&#8230;,z^{(m)}\}\)</span>。做最大对数似然就是要求 <span class="math">\(\theta\)</span>&nbsp;的“最优值”：</p>
<div class="math">
\begin{equation*}
\theta=\arg\max_\theta{L(\theta;X)}
\end{equation*}
</div>
<p>其中</p>
<div class="math">
\begin{equation*}
L(\theta;X)=log{P(X;\theta)}=\log{\sum_Z P(X,Z;\theta)}
\end{equation*}
</div>
<p>想用这个 <span class="math">\(\log\)</span> 套 <span class="math">\(\sum\)</span> 的形式直接求解 <span class="math">\(\theta\)</span> 往往非常困难。而如果能观察到隐含变量 <span class="math">\(z\)</span>&nbsp;，求下面的似然函数的极大值会容易许多：</p>
<div class="math">
\begin{equation*}
L(\theta;X,Z)=\log{P(X, Z;\theta)}
\end{equation*}
</div>
<p>问题是实际上我们没法观察到 <span class="math">\(z\)</span> 的值，只能在给定 <span class="math">\(\theta\)</span> 时求 <span class="math">\(z\)</span> 的后验概率 <span class="math">\(P(z|x;\theta)\)</span> 。  <a class="footnote-reference" href="#id8" id="id2">[1]</a> <span class="caps">EM</span>&nbsp;算法就可以帮我们打破这样的困境。</p>
</div>
<div class="section" id="id3">
<h2>算法</h2>
<p>下面给出 <span class="caps">EM</span> 算法的过程。其中 <span class="math">\(\theta_t\)</span> 是第 t-1 次迭代时算出的 <span class="math">\(\theta\)</span> 值；<span class="math">\(\theta_0\)</span>&nbsp;为任意初值。</p>
<blockquote>
<p>Repeat until converge&nbsp;{</p>
<blockquote>
<ol class="arabic">
<li><p class="first">(E-step) 计算 <span class="math">\(P(Z|X;\theta_t)\)</span>&nbsp;以得到</p>
<div class="math">
\begin{align*}
E_{Z|X;\theta_t}[L(\theta;X,Z)]
&amp;:= E_{Z|X;\theta_t}[\log{P(X,Z;\theta)}] &#92;&#92;
&amp;= \sum_Z P(Z|X;\theta_t) \log{P(X,Z;\theta)}
\end{align*}
</div>
</li>
<li><p class="first">(M-step)</p>
<div class="math">
\begin{equation*}
\theta_{t+1} := \arg\max_\theta E_{Z|X;\theta_t}[\log{P(X,Z;\theta)}]
\end{equation*}
</div>
</li>
</ol>
</blockquote>
<p>}</p>
</blockquote>
<p>对，就这么短。所以我总觉得称之为 algorithm 不如称之为 method 更恰当。上面的过程在收敛后就得到了我们需要的 <span class="math">\(\theta=\arg\max_\theta{L(\theta;X)}\)</span>  <a class="footnote-reference" href="#id9" id="id4">[2]</a>。</p>
<p>先简单说说这短短两步都做了些啥。<span class="caps">EM</span> 算法每次迭代都建立在上轮迭代对 <span class="math">\(\theta\)</span> 的最优值的估计 <span class="math">\(\theta_t\)</span> 上，利用它可以求出 <span class="math">\(Z\)</span> 的后验概率 <span class="math">\(P(Z|X;\theta_t)\)</span> ，进而求出 <span class="math">\(L(\theta;X,Z)\)</span> 在分布 <span class="math">\(Z \sim P(Z|X;\theta)\)</span> 上的期望 <span class="math">\(E_{Z|X;\theta_t}[L(\theta;X,Z)]\)</span>。在第一节中我们提到 <span class="math">\(\arg\max_\theta L(\theta;X,Z)\)</span> 在未知 <span class="math">\(Z\)</span> 的情况下难以直接计算，于是 <span class="caps">EM</span> 算法就转而通过最大化它的期望 <span class="math">\(E_{Z|X;\theta_t}[L(\theta;X,Z)]\)</span> 来逼近 <span class="math">\(\theta\)</span> 的最优值，得到 <span class="math">\(\theta_{t+1}\)</span> 。注意由于 <span class="math">\(L(\theta;X,Z)\)</span> 的这个期望是在 <span class="math">\(Z\)</span> 的一个分布上求的，这样得到的表达式就只剩下 <span class="math">\(\theta\)</span> 一个未知量，因而绕过了 <span class="math">\(z\)</span> 未知的问题。而 <span class="math">\(\theta_{t+1}\)</span> 又可以作为下轮迭代的基础，继续向最优逼近。算法中 E-step 就是在利用 <span class="math">\(\theta_t\)</span> 求期望 <span class="math">\(E_{Z|X;\theta_t}[L(\theta;X,Z)]\)</span>，这就是所谓“Expectation”；M-step 就是通过寻找 <span class="math">\(\theta_{t+1}\)</span> 最大化这个期望来逼近 <span class="math">\(\theta\)</span> 的最优值，这就叫“Maximization”。<span class="caps">EM</span>&nbsp;算法因此得名。</p>
<p>另外，如果数据满足独立同分布的条件，分别枚举 <span class="math">\(z^{(i)}\)</span> 的值可能要比枚举整个 <span class="math">\(Z\)</span> 方便些，可把 <span class="math">\(E_{Z|X;\theta_t}[L(\theta;X,Z)]\)</span>&nbsp;替换成：</p>
<div class="math">
\begin{align*}
\sum_i E_{z^{(i)}|x^{(i)};\theta_t}[L(\theta;x^{(i)},z^{(i)}]
&amp;:= \sum_i E_{z^{(i)}|x^{(i)};\theta_t}[\log{P(x^{(i)},z^{(i)};\theta)}] &#92;&#92;
&amp;= \sum_i \sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\theta_t) \log{P(x^{(i)},z^{(i)};\theta)}
\end{align*}
</div>
</div>
<div class="section" id="id5">
<h2>原理</h2>
<p>为什么这样 E一步，M一步，一步E，一步M，就能逼近极大似然？具体而言，为什么通过迭代计算 <span class="math">\(\arg\max_\theta E_{Z|X;\theta_t}[L(\theta;X,Z)]\)</span> 可以逼近 <span class="math">\(\theta\)</span> 的最优值 <span class="math">\(\arg\max_\theta L(\theta;X,Z)\)</span>？我们稍后会看到，这是因为每次迭代得到的 <span class="math">\(\theta_{t+1}\)</span> 一定比 <span class="math">\(\theta_t\)</span> 更优，即算法是在对 <span class="math">\(\theta\)</span>&nbsp;的最优值做单调逼近。</p>
<p>不过首先让我们先抛开最大似然，考虑一个更一般的问题。假设有一个凹函数 <span class="math">\(F(\theta)\)</span> ，我们想求 <span class="math">\(\arg\max_\theta F(\theta)\)</span> ，但直接求很困难。不过对于任意给定的 <span class="math">\(\theta_t\)</span>，假设我们都能找到 <span class="math">\(F(\theta)\)</span> 的一个下界函数 <span class="math">\(G_{\theta_t}(\theta)\)</span>，满足 <span class="math">\(F(\theta) \geq G_{\theta_t}(\theta)\)</span> 且 <span class="math">\(F(\theta_t) = G_{\theta_t}(\theta_t)\)</span> ——我管 <span class="math">\(G_{\theta_t}(\theta)\)</span> 这样的函数叫 <span class="math">\(F\)</span> 的“在 <span class="math">\(\theta_t\)</span> 处相等的下界函数”。现在考虑 <span class="math">\(\theta_{t+1} := \arg\max_\theta G_{\theta_t}(\theta)\)</span>，它一定会满足：</p>
<div class="math">
\begin{equation*}
F(\theta_{t+1}) \geq G_{\theta_t}(\theta_{t+1}) \geq G_{\theta_t}(\theta_t) = F(\theta_t)
\end{equation*}
</div>
<p>也就是说， <span class="math">\(\theta_{t+1}\)</span> 一定比 <span class="math">\(\theta_t\)</span> 更优。而接下来我们又可以用 <span class="math">\(\theta_{t+1}\)</span> 找到一个 <span class="math">\(G_{\theta_{t+1}}(\theta)\)</span>，再据此算出比 <span class="math">\(\theta_{t+1}\)</span> 还优的 <span class="math">\(\theta_{t+2} := \arg\max_\theta G_{\theta_{t+1}}(\theta)\)</span> 。如此不断迭代，就能不断步步逼近 <span class="math">\(\theta\)</span> 的最优值了。由此可见，如果对任意 <span class="math">\(\theta_t\)</span> 都能找到 <span class="math">\(F\)</span> 的“在 <span class="math">\(\theta_t\)</span> 处相等的下界函数” <span class="math">\(G_{\theta_t}(\theta)\)</span>，我们就得到了一个能单调逼近 <span class="math">\(\theta\)</span>&nbsp;的最优值的算法：</p>
<blockquote>
<p>Repeat until converge&nbsp;{</p>
<blockquote>
<ol class="arabic">
<li><p class="first">找到函数 <span class="math">\(F(\theta)\)</span> 的“在 <span class="math">\(\theta_t\)</span> 处相等的下界函数” <span class="math">\(G_{\theta_t}(\theta)\)</span></p>
</li>
<li><p class="first">更新参数</p>
<div class="math">
\begin{equation*}
\theta_{t+1} := \arg\max_\theta G_{\theta_t}(\theta)
\end{equation*}
</div>
</li>
</ol>
</blockquote>
<p>}</p>
</blockquote>
<p>上面的算法看起来和 <span class="caps">EM</span> 算法的每步都分别对应——事实上也正如此。下面是从 <a class="reference external" href="http://research.microsoft.com/en-us/um/people/cmbishop/prml/"><span class="caps">PRML</span></a>&nbsp;中偷的一张图改的，展示了上述逼近的过程：</p>
<p><img alt="image0" src="/images/2011-10-13-em-algo.png" /></p>
<p>现在我们回到最大似然问题 <span class="math">\(\theta=\arg\max_\theta{L(\theta;X)}\)</span> 。如果我们想套用上面的算法来逼近 <span class="math">\(\theta\)</span> 的这个最优解，就需要找到对于每个 <span class="math">\(\theta_t\)</span>，函数 <span class="math">\(L(\theta;X)\)</span> 的“在 <span class="math">\(\theta_t\)</span> 处相等的下界函数”。该怎么找呢？让我们从 <span class="math">\(L(\theta)\)</span>&nbsp;的初始形式开始推导：</p>
<div class="math">
\begin{align*}
L(\theta)
&amp;= \log{P(X;\theta)} &#92;&#92;
&amp;= \log{\sum_Z P(X,Z;\theta)}
\end{align*}
</div>
<p>又卡在这个 <span class="math">\(\log\)</span> 套 <span class="math">\(\sum\)</span> 的形式上了……我们说过麻烦在于观察不到 <span class="math">\(Z\)</span> 的值，那不妨给它任意引入一个概率分布 <span class="math">\(Q(Z)\)</span>  <a class="footnote-reference" href="#id10" id="id6">[3]</a>，利用分子分母同乘 <span class="math">\(Q(Z)\)</span> 的小&nbsp;trick，得到：</p>
<div class="math">
\begin{align*}
L(\theta)
&amp;= \log{\sum_Z P(X,Z;\theta)} &#92;&#92;
&amp;= \log{\sum_Z Q(Z) \frac{P(X,Z;\theta)}{Q(Z)}} &#92;&#92;
&amp;= \log E_{Z \sim Q}\left[ \frac{P(X,Z;\theta)}{Q(Z)} \right]
\end{align*}
</div>
<p>根据 Jensen 不等式  <a class="footnote-reference" href="#id11" id="id7">[4]</a>，对于任意分布 <span class="math">\(Q\)</span>&nbsp;都有：</p>
<div class="math">
\begin{equation*}
L(\theta) = \log E_{Z \sim Q}\left[ \frac{P(X,Z;\theta)}{Q(Z)} \right] \geq E_{Z \sim Q}\left[ \log\frac{P(X,Z;\theta)}{Q(Z)} \right]
\end{equation*}
</div>
<p>且上面的不等式在 <span class="math">\(\frac {P(X,Z;\theta)} {Q(Z)}\)</span>&nbsp;为常数时取等号。</p>
<p>于是我们就得到了 <span class="math">\(L(\theta;X)\)</span> 的一个下界函数。我们要想套用上面的算法，还要让这个不等式在 <span class="math">\(\theta_t\)</span> 处取等号，这就这要求在 <span class="math">\(\theta = \theta_t\)</span> 时 <span class="math">\(\frac {P(X,Z;\theta)} {Q(Z)}\)</span> 为常数，即 <span class="math">\(Q(Z) \propto P(X,Z;\theta_t)\)</span>。由于 <span class="math">\(Q(Z)\)</span> 是一个概率分布，必须满足 <span class="math">\(\sum_z Q_i(z) = 1\)</span>，所以这样的 <span class="math">\(Q(Z)\)</span> 只能是 <span class="math">\(Q(Z) = \frac {P(X,Z;\theta_t)} {\sum_Z P(X,Z;\theta_t)} = P(Z|X;\theta_t)\)</span>。那我们就把 <span class="math">\(Q(Z) = P(Z|X;\theta_t)\)</span>&nbsp;代入上式，得到：</p>
<div class="math">
\begin{equation*}
L(\theta) \geq E_{Z|X;\theta_t}\left[ \log\frac{P(X,Z;\theta)}{P(Z|X;\theta_t)} \right]
\end{equation*}
</div>
<p>且该不等式在 <span class="math">\(\theta = \theta_t\)</span> 时取到等号。那么…… <span class="math">\(E_{Z|X;\theta_t}\left[ \log\frac{P(X,Z;\theta)}{P(Z|X;\theta_t)} \right]\)</span> 就是 <span class="math">\(L(\theta;X)\)</span> 的“在 <span class="math">\(\theta_t\)</span> 处相等的下界函数”——这不就是我们要找的么！于是把它塞进本节开始得到的算法里替换“ <span class="math">\(G_{\theta_t}(\theta)\)</span> ”就可以用啦。也就是说，迭代计算 <span class="math">\(\arg\max_\theta E_{Z|X;\theta_t}\left[ \log\frac{P(X,Z;\theta)}{P(Z|X;\theta_t)} \right]\)</span>就可以逼近 <span class="math">\(\theta\)</span> 的最优值了。而由于利用 Jensen 不等式的那一步搞掉了<span class="math">\(\log\)</span>套<span class="math">\(\sum\)</span>的形式，它算起来往往要比直接算 <span class="math">\(\arg\max_\theta{L(\theta;X)}\)</span>&nbsp;容易不少。</p>
<p>我们还可以再做几步推导，得到一个更简单的形式：</p>
<div class="math">
\begin{align*}
\theta_{t+1}
&amp;:= \arg\max_\theta E_{Z|X;\theta_t}\left[ \log\frac{P(X,Z;\theta)}{P(Z|X;\theta_t)} \right] &#92;&#92;
&amp;\equiv \arg\max_\theta \sum_{Z} P(Z|X;\theta_t) \log\frac{P(X,Z;\theta)}{P(Z|X;\theta_t)} &#92;&#92;
&amp;= \arg\max_\theta \sum_{Z} [P(Z|X;\theta_t)\log P(X,Z;\theta) - P(Z|X;\theta_t) \log P(Z|X;\theta_t)] &#92;&#92;
&amp;= \arg\max_\theta \sum_{Z} P(Z|X;\theta_t)\log P(X,Z;\theta) &#92;&#92;
&amp;\equiv \arg\max_\theta E_{Z|X;\theta_t}[\log{P(X,Z;\theta)}]
\end{align*}
</div>
<p>其中倒数第二步是因为 <span class="math">\(- P(Z|X;\theta_t) \log P(Z|X;\theta_t)]\)</span> 这一项与 <span class="math">\(\theta\)</span> 无关，所以就直接扔掉了。这样就得到了本文第二节 <span class="caps">EM</span>&nbsp;算法中的形式——它就是这么来的。</p>
<p>以上就是 <span class="caps">EM</span>&nbsp;了。至于独立同分布的情况推导也类似。</p>
<p>顺带一提，<span class="math">\(\arg\max_\theta E_{Z|X;\theta_t}[\log{P(X,Z;\theta)}]\)</span> 有时也比较难算。这时我们其实可以退而求其次，不要求这个期望最大化了，只要它在 <span class="math">\(\theta_{t+1}\)</span> 处的值大于在 <span class="math">\(\theta_t\)</span> 处的值就行了。根据上面的推导，这样也能逼近 <span class="math">\(\theta\)</span> 的最优值，只是收敛速度较慢。这就是所谓 <span class="caps">GEM</span> (Generalized <span class="caps">EM</span>)&nbsp;算法了。</p>
<p>p.s. <a class="reference external" href="http://www.mathjax.org/">MathJax</a>&nbsp;很神嘛。</p>
<p>p.p.s. 这篇笔记竟然断断续续写写改改了两天多，我对 <span class="caps">EM</span> 的认识也越来越清晰。“<a class="reference external" href="http://mindhacks.cn/2009/02/15/why-you-should-start-blogging-now/">‘教’是最好的‘学’</a>”真是一点没错。</p>
<table class="docutils footnote" frame="void" id="id8" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2">[1]</a></td><td><p class="first">一般可以利用<a class="reference external" href="http://en.wikipedia.org/wiki/Bayes%27_theorem">贝叶斯定理</a>：</p>
<div class="math">
\begin{equation*}
P(z|x;\theta) = \frac{P(x|z;\theta)P(z;\theta)}{\sum_z{P(x|z;\theta)P(z;\theta)}}
\end{equation*}
</div>
<p class="last">而 <span class="math">\(P(x|z;\theta)\)</span> 和 <span class="math">\(P(z;\theta)\)</span>&nbsp;往往是模型假设的一部分。</p>
</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id9" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id4">[2]</a></td><td>实际上在某些特殊情况下，<span class="math">\(\theta\)</span> 还可能收敛在局部最优点或鞍点上。这时可以多跑几次算法，每次随机不同的 <span class="math">\(\theta_0\)</span>，最后取最好的结果。为简明起见，本文忽略这种情况。</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id10" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id6">[3]</a></td><td><span class="math">\(Q(Z)\)</span> 为概率分布，意即需满足 <span class="math">\(\sum_Z Q(Z) = 1\)</span> 且 <span class="math">\(Q(Z) \geq 0\)</span></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id11" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id7">[4]</a></td><td><p class="first">Jensen&nbsp;不等式：</p>
<p><span class="math">\(f\)</span> 为凸函数，<span class="math">\(X\)</span>&nbsp;为随机变量。则</p>
<div class="math">
\begin{equation*}
E[f(X)]\geq f(E[X])
\end{equation*}
</div>
<p>若 <span class="math">\(f\)</span> 是严格凸的，则上式取等号当前仅当 <span class="math">\(X\)</span>&nbsp;为常数。</p>
<p class="last">在这里 <span class="math">\(\log\)</span>&nbsp;函数是严格凹的，所以要把上面的不等号方向调转。</p>
</td></tr>
</tbody>
</table>
</div>

  </div>

  <div class="comments">
    <h3>Comments</h3>
    <div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_identifier = "2011/10/em-algorithm/";
      (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://tomtung.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
  </div>
</section>
  </div>


   <div id="page-footer-container">

    <footer id="page-footer" class="row">
      <div class="six columns">
               <address id="about" class="vcard body">
                  Copyright © 逆铭 (tomtung) 2012 
                  <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/">
                    <img id="license-image-80x15" alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-nc-sa/3.0/80x15.png" />
                  </a>
                  <br />
                  Proudly powered by <a href="http://getpelican.com/">Pelican</a> and the <a target="_blank" href="http://gumbyframework.com">Gumby Framework</a>.
              </address>
      </div>

      <div class="six columns">
        <div class="row">
          <ul id="socbtns">

            <li><div class="btn primary"><a href="https://github.com/tomtung" target="_blank">Github</a></div></li>

            <li><div class="btn twitter"><a href="https://twitter.com/tomtung" target="_blank">Twitter</a></div></li>



            <li><div class="btn weibo"><a href="http://www.weibo.com/1245986775" target="_blank">Weibo</a></div></li>

            <li><div class="btn douban"><a href="http://www.douban.com/people/tomtung" target="_blank">Douban</a></div></li>

          </ul>
        </div>
      </div>
    </footer>

    </div>


<script type="text/javascript">
    var disqus_shortname = 'tomtung';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>

  <!-- Grab Google CDN's jQuery, fall back to local if offline -->
  <!-- 2.0 for modern browsers, 1.10 for .oldie -->
  <script>
    var oldieCheck = Boolean(document.getElementsByTagName('html')[0].className.match(/\soldie\s/g));
    if(!oldieCheck) {
    document.write('<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.2/jquery.min.js"><\/script>');
    } else {
    document.write('<script src="//ajax.googleapis.com/ajax/libs/jquery/1.10.1/jquery.min.js"><\/script>');
    }
  </script>
  <script>
    if(!window.jQuery) {
    if(!oldieCheck) {
      document.write('<script src="/bower_components/gumby/js/libs/jquery-2.0.2.min.js"><\/script>');
    } else {
      document.write('<script src="/bower_components/gumby/js/libs/jquery-1.10.1.min.js"><\/script>');
    }
    }
  </script>

  <script src="http://blog.tomtung.com/theme/js/libs/gumby.min.js"></script>
  <script src="http://blog.tomtung.com/theme/js/plugins.js"></script>

  <!-- Prompt IE 6 users to install Chrome Frame. Remove this if you want to support IE 6.
   chromium.org/developers/how-tos/chrome-frame-getting-started -->
  <!--[if lt IE 7 ]>
  <script src="//ajax.googleapis.com/ajax/libs/chrome-frame/1.0.3/CFInstall.min.js"></script>
  <script>window.attachEvent('onload',function(){CFInstall.check({mode:'overlay'})})</script>
  <![endif]-->

</body>
</html>